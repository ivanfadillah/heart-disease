
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Heart-Disease}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Heart Disease Model
Construction}\label{heart-disease-model-construction}

Kelompok AI-ven William Juniarta Hadiman - 13516026 Mochammad Alghifari
- 13516038 Dion Saputra - 13516045 Rifo Ahmad Genadi - 13516111 Ivan
Fadillah - 13516128

    \subsubsection{Import Necessary Library}\label{import-necessary-library}

Library yang digunakan pada pembentukan model ini adalah scikit-learn,
pandas, numpy, dan itertools. Scikit-learn digunakan untuk training
model, pandas digunakan untuk menampung data ke dalam dataframe, numpy
digunakan pada penanganan missing values, dan itertools digunakan untuk
mengiterasi kombinasi yang mungkin dari feature yang ada

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}103}]:} \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
          \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
          \PY{k+kn}{import} \PY{n+nn}{itertools}
          \PY{k+kn}{import} \PY{n+nn}{math}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{externals} \PY{k}{import} \PY{n}{joblib}
          \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{preprocessing}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{impute} \PY{k}{import} \PY{n}{SimpleImputer}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{naive\PYZus{}bayes} \PY{k}{import} \PY{n}{GaussianNB}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{accuracy\PYZus{}score}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{precision\PYZus{}score}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{recall\PYZus{}score}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{KFold}
          \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{tree}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{neighbors} \PY{k}{import} \PY{n}{KNeighborsClassifier}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{neural\PYZus{}network} \PY{k}{import} \PY{n}{MLPClassifier}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{classification\PYZus{}report}
\end{Verbatim}


    \subsubsection{Import Dataset}\label{import-dataset}

Dataset yang digunakan berasal dari file
tubes2\_HeartDisease\_train.csv. Digunakan library pandas untuk membaca
file csv tersebut ke dalam dataframe pandas

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}96}]:} \PY{n}{file} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{tubes2\PYZus{}HeartDisease\PYZus{}train.csv}\PY{l+s+s2}{\PYZdq{}}
         \PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{n}{file}\PY{p}{)}
         
         \PY{n}{feature} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Column14}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{inplace}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{label} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Column14}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
         
         \PY{n}{df}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}96}]:}    Column1  Column2  Column3 Column4 Column5 Column6 Column7 Column8 Column9  \textbackslash{}
         0       54        1        4     125     216       0       0     140       0   
         1       55        1        4     158     217       0       0     110       1   
         2       54        0        3     135     304       1       0     170       0   
         3       48        0        3     120     195       0       0     125       0   
         4       50        1        4     120       0       0       1     156       1   
         
           Column10 Column11 Column12 Column13  Column14  
         0        0        ?        ?        ?         1  
         1      2.5        2        ?        ?         1  
         2        0        1        0        3         0  
         3        0        ?        ?        ?         0  
         4        0        1        ?        6         3  
\end{Verbatim}
            
    \subsubsection{Handling Missing Values}\label{handling-missing-values}

Pada dataset training yang digunakan terdapat missing values pada
beberapa atribut. Missing values ditandai dengan character '?. Missing
values tersebut perlu ditangani agar dapat dijalankan pada model yang
akan digunakan. Pada eksplorasi ini, missing values diganti dengan nilai
mean atau modus dari atribut tersebut. Penggantian dengan nilai mean
digunakan untuk atribut dengan value kontinu, sedangkan nilai modus
digunakan untuk atribut dengan value diskrit. Handling values hanya
dilakukan untuk atribut yang merubakan feature dari data

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}97}]:} \PY{n}{header} \PY{o}{=} \PY{n}{feature}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}
         \PY{n}{feature\PYZus{}impute} \PY{o}{=} \PY{n}{feature}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{?}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{nan}\PY{p}{)}
         
         \PY{n}{imputer\PYZus{}mode} \PY{o}{=} \PY{n}{SimpleImputer}\PY{p}{(}\PY{n}{missing\PYZus{}values}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{nan}\PY{p}{,} \PY{n}{strategy}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{most\PYZus{}frequent}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{imputer\PYZus{}mean} \PY{o}{=} \PY{n}{SimpleImputer}\PY{p}{(}\PY{n}{missing\PYZus{}values}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{nan}\PY{p}{,} \PY{n}{strategy}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{discrete\PYZus{}value} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Column1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Column2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Column3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Column6}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Column7}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Column9}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Column13}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{continues\PYZus{}value} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Column4}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Column5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Column8}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Column10}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Column11}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Column12}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         
         \PY{n}{imputer\PYZus{}mode}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{feature\PYZus{}impute}\PY{p}{[}\PY{n}{discrete\PYZus{}value}\PY{p}{]}\PY{p}{)}
         \PY{n}{feature\PYZus{}impute}\PY{p}{[}\PY{n}{discrete\PYZus{}value}\PY{p}{]} \PY{o}{=} \PY{n}{imputer\PYZus{}mode}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{feature\PYZus{}impute}\PY{p}{[}\PY{n}{discrete\PYZus{}value}\PY{p}{]}\PY{p}{)}
         
         \PY{n}{imputer\PYZus{}mean}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{feature\PYZus{}impute}\PY{p}{[}\PY{n}{continues\PYZus{}value}\PY{p}{]}\PY{p}{)}
         \PY{n}{feature\PYZus{}impute}\PY{p}{[}\PY{n}{continues\PYZus{}value}\PY{p}{]} \PY{o}{=} \PY{n}{imputer\PYZus{}mean}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{feature\PYZus{}impute}\PY{p}{[}\PY{n}{continues\PYZus{}value}\PY{p}{]}\PY{p}{)}
         
         \PY{n}{feature\PYZus{}impute}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Column13}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{to\PYZus{}numeric}\PY{p}{(}\PY{n}{feature\PYZus{}impute}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Column13}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{feature\PYZus{}impute}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}97}]:}    Column1  Column2  Column3  Column4  Column5 Column6 Column7  Column8  \textbackslash{}
         0       54        1        4    125.0    216.0       0       0    140.0   
         1       55        1        4    158.0    217.0       0       0    110.0   
         2       54        0        3    135.0    304.0       1       0    170.0   
         3       48        0        3    120.0    195.0       0       0    125.0   
         4       50        1        4    120.0      0.0       0       1    156.0   
         
           Column9  Column10  Column11  Column12  Column13  
         0       0       0.0  1.762089  0.686792         3  
         1       1       2.5  2.000000  0.686792         3  
         2       0       0.0  1.000000  0.000000         3  
         3       0       0.0  1.762089  0.686792         3  
         4       1       0.0  1.000000  0.686792         6  
\end{Verbatim}
            
    \subsubsection{Penanganan Outlier}\label{penanganan-outlier}

Outlier merupakan instance yang tidak normal terhadap data lainnya.
Outlier perlu dihilangkan karena dikhawatirkan instance tersebut
didapatkan dari pengukuran yang bersifat noisy. Pada eksplorasi ini,
sebuah instance disebut outlier jika value pada atribut tertentu tidak
berada dalam range \(\mu \pm \sigma\).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}98}]:} \PY{n}{idx\PYZus{}to\PYZus{}drop} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         
         \PY{k}{for} \PY{n}{item} \PY{o+ow}{in} \PY{n}{continues\PYZus{}value}\PY{p}{:}
             \PY{n}{mean} \PY{o}{=} \PY{n}{feature\PYZus{}impute}\PY{p}{[}\PY{n}{item}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
             \PY{n}{std} \PY{o}{=} \PY{n}{feature\PYZus{}impute}\PY{p}{[}\PY{n}{item}\PY{p}{]}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{p}{)}
             \PY{n}{low\PYZus{}threshold} \PY{o}{=} \PY{n}{mean} \PY{o}{\PYZhy{}} \PY{l+m+mi}{2}\PY{o}{*}\PY{n}{std}
             \PY{n}{high\PYZus{}threshold} \PY{o}{=} \PY{n}{mean} \PY{o}{+} \PY{l+m+mi}{2}\PY{o}{*}\PY{n}{std}
                 
             \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{feature\PYZus{}impute}\PY{p}{[}\PY{n}{item}\PY{p}{]}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{:}
                 \PY{n}{cur\PYZus{}value} \PY{o}{=} \PY{n}{feature\PYZus{}impute}\PY{p}{[}\PY{n}{item}\PY{p}{]}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{n}{i}\PY{p}{]}
                 \PY{k}{if} \PY{p}{(}\PY{n}{cur\PYZus{}value} \PY{o}{\PYZlt{}} \PY{n}{low\PYZus{}threshold} \PY{o+ow}{or} \PY{n}{cur\PYZus{}value} \PY{o}{\PYZgt{}} \PY{n}{high\PYZus{}threshold}\PY{p}{)}\PY{p}{:}
                     \PY{n}{idx\PYZus{}to\PYZus{}drop}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{i}\PY{p}{)}
         
         \PY{n}{feature\PYZus{}impute}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{feature\PYZus{}impute}\PY{o}{.}\PY{n}{index}\PY{p}{[}\PY{n}{idx\PYZus{}to\PYZus{}drop}\PY{p}{]}\PY{p}{,}\PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         \PY{n}{label}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{label}\PY{o}{.}\PY{n}{index}\PY{p}{[}\PY{n}{idx\PYZus{}to\PYZus{}drop}\PY{p}{]}\PY{p}{,}\PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         
         \PY{n}{feature\PYZus{}impute}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}98}]:}           Column1     Column2     Column3     Column4     Column5     Column8  \textbackslash{}
         count  598.000000  598.000000  598.000000  598.000000  598.000000  598.000000   
         mean    52.369565    0.764214    3.217391  130.387932  200.704864  140.356731   
         std      9.233553    0.424844    0.918577   14.153952  102.705054   23.381618   
         min     28.000000    0.000000    1.000000   96.000000    0.000000   88.000000   
         25\%     46.000000    1.000000    2.000000  120.000000  183.250000  122.000000   
         50\%     53.000000    1.000000    4.000000  130.000000  221.500000  140.000000   
         75\%     59.000000    1.000000    4.000000  140.000000  268.000000  159.000000   
         max     77.000000    1.000000    4.000000  165.000000  412.000000  188.000000   
         
                  Column10    Column11    Column12    Column13  
         count  598.000000  598.000000  598.000000  598.000000  
         mean     2.347483    1.663715    0.567426    3.794314  
         std      4.142096    0.388179    0.301544    1.557228  
         min     -2.600000    1.000000    0.000000    3.000000  
         25\%      0.000000    1.762089    0.686792    3.000000  
         50\%      0.500000    1.762089    0.686792    3.000000  
         75\%      2.950000    2.000000    0.686792    3.000000  
         max     19.000000    2.000000    1.000000    7.000000  
\end{Verbatim}
            
    \subsubsection{Feature Scalling}\label{feature-scalling}

Pada dataset yang digunakan antar-atribut pada feature memiliki range
nilai yang berbeda. Terdapat atribut dengan range nilai satuan dan
terdapat pula atribut dengan range nilai ratusan. Perbedaan range ini
dapat menyebabkan atribut dengan range besar memiliki kontribusi yang
besar terhadap perhitungan model. Feature Scalling akan merubah nilai
atribut tersebut sedemikian sehingga range pada setiap atribut tersebut
sama.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}99}]:} \PY{n}{feature\PYZus{}scale} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{preprocessing}\PY{o}{.}\PY{n}{scale}\PY{p}{(}\PY{n}{feature\PYZus{}impute}\PY{p}{)}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{n}{header}\PY{p}{)}
         \PY{n}{feature\PYZus{}scale}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/home/ds/anaconda3/lib/python3.6/site-packages/ipykernel\_launcher.py:1: DataConversionWarning: Data with input dtype int64, float64, object were all converted to float64 by the scale function.
  """Entry point for launching an IPython kernel.

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}99}]:}     Column1   Column2   Column3   Column4   Column5   Column6   Column7  \textbackslash{}
         0  0.176725  0.555458  0.852692 -0.380985  0.149048 -0.387298 -0.693683   
         1  0.285116  0.555458  0.852692  1.952471  0.158792 -0.387298 -0.693683   
         2  0.176725 -1.800315 -0.236859  0.326123  1.006587  2.581989 -0.693683   
         3 -0.473623 -1.800315 -0.236859 -0.734539 -0.055593 -0.387298 -0.693683   
         4 -0.256840  0.555458  0.852692 -0.734539 -1.955823 -0.387298  0.619047   
         
             Column8   Column9  Column10  Column11  Column12  Column13  
         0 -0.015270 -0.700908 -0.567213  0.253637  0.396182 -0.510509  
         1 -1.299403  1.426721  0.036852  0.867040  0.396182 -0.510509  
         2  1.268864 -0.700908 -0.567213 -1.711246 -1.883308 -0.510509  
         3 -0.657336 -0.700908 -0.567213  0.253637  0.396182 -0.510509  
         4  0.669601  1.426721 -0.567213 -1.711246  0.396182  1.417604  
\end{Verbatim}
            
    \subsubsection{Feature Selection}\label{feature-selection}

Feature Selection digunakan untuk memilih feature yang memiliki pengaruh
terhadap kinerja model. Feature selection yang digunakan adalah Forward
Selection. Sebelumnya didaftarkan terlebih dahulu semua kemungkinan dari
kombinasi feature yang mungkin. Lalu dimulai dari 0 feature, kinerja
model dihitung. Kemudian jumlah feature ditambahkan hingga kinerja model
tidak meningkat lagi.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}100}]:} \PY{n}{feature\PYZus{}combinations} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          
          \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{14}\PY{p}{)}\PY{p}{:}
              \PY{n}{feature\PYZus{}combinations}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n}{itertools}\PY{o}{.}\PY{n}{combinations}\PY{p}{(}\PY{n}{header}\PY{p}{,}\PY{n}{i}\PY{p}{)}\PY{p}{)}\PY{p}{)}  
\end{Verbatim}


    \subsubsection{Ukuran Kinerja Model}\label{ukuran-kinerja-model}

    Ukuran kinerja model akan ditentukan berdasarkan nilai accuracy,
precission, dan recall dari model yang dihasilkan.

    \subsubsection{K-Nearest Neighboard}\label{k-nearest-neighboard}

    K-Nearest neighboard dibangun dengan memilih nilai k yang mengoptimalkan
kinerja KNN dengan mengiterasi k dari 1 hingga \(\sqrt(n)\). Untuk split
datanya digunakan metode 10-fold. Berdasarkan eksperimen diperoleh nilai
k yang optimal yaitu untuk k = 6 dengan feature yang dipilih adalah
{[}'Column2', 'Column3', 'Column4', 'Column5', 'Column6', 'Column8',
'Column9'{]}. Model ini mencapai akurasi 80\%

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}102}]:} \PY{n}{kf} \PY{o}{=} \PY{n}{KFold}\PY{p}{(}\PY{n}{n\PYZus{}splits}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
          
          \PY{n}{choosen\PYZus{}k} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{;}
          \PY{n}{best\PYZus{}accuracy\PYZus{}knn} \PY{o}{=} \PY{l+m+mf}{0.0}
          \PY{n}{best\PYZus{}header\PYZus{}knn} \PY{o}{=} \PY{k+kc}{None}
          
          \PY{n}{best\PYZus{}label\PYZus{}knn} \PY{o}{=} \PY{k+kc}{None}
          \PY{n}{best\PYZus{}predict\PYZus{}knn} \PY{o}{=} \PY{k+kc}{None}
          
          \PY{n}{i} \PY{o}{=} \PY{l+m+mi}{7}    \PY{c+c1}{\PYZsh{} jumlah feature yang digunakan}
          \PY{n}{s} \PY{o}{=} \PY{l+m+mi}{149}
          \PY{n}{e} \PY{o}{=} \PY{l+m+mi}{150}
          
          \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{s}\PY{p}{,}\PY{n}{e}\PY{p}{)}\PY{p}{:}
              \PY{n}{cur\PYZus{}header} \PY{o}{=} \PY{p}{[}\PY{p}{]}
              \PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{feature\PYZus{}combinations}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                  \PY{n}{cur\PYZus{}header}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{feature\PYZus{}combinations}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{[}\PY{n}{k}\PY{p}{]}\PY{p}{)}
          
              \PY{n}{cur\PYZus{}feature} \PY{o}{=} \PY{n}{feature\PYZus{}scale}\PY{p}{[}\PY{n}{cur\PYZus{}header}\PY{p}{]}
              \PY{k}{for} \PY{n}{train\PYZus{}idx}\PY{p}{,}\PY{n}{test\PYZus{}idx} \PY{o+ow}{in} \PY{n}{kf}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{n}{cur\PYZus{}feature}\PY{p}{)}\PY{p}{:}
                  \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{cur\PYZus{}feature}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{n}{train\PYZus{}idx}\PY{p}{]} 
                  \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{label}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{n}{train\PYZus{}idx}\PY{p}{]}       
          
                  \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{cur\PYZus{}feature}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{n}{test\PYZus{}idx}\PY{p}{]}
                  \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{label}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{n}{test\PYZus{}idx}\PY{p}{]} 
                  
                  \PY{k}{for} \PY{n}{t} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{n+nb}{int}\PY{p}{(}\PY{n}{math}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                      \PY{n}{cur\PYZus{}model} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}neighbors}\PY{o}{=}\PY{n}{t}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
                      \PY{n}{y\PYZus{}predict} \PY{o}{=} \PY{n}{cur\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
                      \PY{n}{cur\PYZus{}accuracy} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}predict}\PY{p}{)}
          
                      \PY{k}{if} \PY{p}{(}\PY{n}{cur\PYZus{}accuracy} \PY{o}{\PYZgt{}} \PY{n}{best\PYZus{}accuracy\PYZus{}knn}\PY{p}{)}\PY{p}{:}
                          \PY{n}{choosen\PYZus{}k} \PY{o}{=} \PY{n}{t}
                          \PY{n}{best\PYZus{}accuracy\PYZus{}knn} \PY{o}{=} \PY{n}{cur\PYZus{}accuracy}
                          \PY{n}{best\PYZus{}header\PYZus{}knn} \PY{o}{=} \PY{n}{cur\PYZus{}header}
                          \PY{n}{best\PYZus{}label\PYZus{}knn} \PY{o}{=} \PY{n}{y\PYZus{}test}
                          \PY{n}{best\PYZus{}predict\PYZus{}knn} \PY{o}{=} \PY{n}{y\PYZus{}predict}
          
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Choosen k: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{choosen\PYZus{}k}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Choosen feature: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{best\PYZus{}header}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{best\PYZus{}accuracy\PYZus{}knn}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Precission: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{precision\PYZus{}score}\PY{p}{(}\PY{n}{best\PYZus{}label\PYZus{}knn}\PY{p}{,}\PY{n}{best\PYZus{}predict\PYZus{}knn}\PY{p}{,}\PY{n}{average}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{micro}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Recall: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{recall\PYZus{}score}\PY{p}{(}\PY{n}{best\PYZus{}label\PYZus{}knn}\PY{p}{,}\PY{n}{best\PYZus{}predict\PYZus{}knn}\PY{p}{,}\PY{n}{average}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{micro}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Choosen k:  6
Choosen feature:  ['Column2', 'Column3', 'Column4', 'Column5', 'Column6', 'Column8', 'Column9']
Accuracy:  0.8
Precission:  0.8
Recall:  0.8

    \end{Verbatim}

    \subsubsection{Naive Bayes}\label{naive-bayes}

Naive Bayes yang digunakan pada eksplorasi ini adalah Gaussian Naive
Bayes. Pembangunan model menggunakan K-Fold dengan 10 Fold sambil
mencari kombinasi feature yang mengoptimalkan accuracy model. Untuk
naive bayes, jumlah feature yang mengoptimalkan kinerja adalah 10
feature yaitu {[}'Column1', 'Column3', 'Column4', 'Column5', 'Column7',
'Column9', 'Column10', 'Column11', 'Column12', 'Column13'{]}. Akurasi
dari model Naive Bayes yang didapatkan yaitu 75\%

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}81}]:} \PY{n}{kf} \PY{o}{=} \PY{n}{KFold}\PY{p}{(}\PY{n}{n\PYZus{}splits}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
         
         \PY{n}{best\PYZus{}model\PYZus{}nb} \PY{o}{=} \PY{k+kc}{None}
         \PY{n}{best\PYZus{}accuracy\PYZus{}nb} \PY{o}{=} \PY{l+m+mf}{0.0}
         \PY{n}{best\PYZus{}header\PYZus{}nb} \PY{o}{=} \PY{k+kc}{None}
         
         \PY{n}{best\PYZus{}label\PYZus{}nb} \PY{o}{=} \PY{k+kc}{None}
         \PY{n}{best\PYZus{}predict\PYZus{}nb} \PY{o}{=} \PY{k+kc}{None}
         
         \PY{n}{i} \PY{o}{=} \PY{l+m+mi}{9}    \PY{c+c1}{\PYZsh{} jumlah feature yang digunakan}
         \PY{n}{s} \PY{o}{=} \PY{l+m+mi}{191}
         \PY{n}{e} \PY{o}{=} \PY{l+m+mi}{192}
         
         \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{s}\PY{p}{,}\PY{n}{e}\PY{p}{)}\PY{p}{:}
             \PY{n}{cur\PYZus{}header} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{feature\PYZus{}combinations}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                 \PY{n}{cur\PYZus{}header}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{feature\PYZus{}combinations}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{[}\PY{n}{k}\PY{p}{]}\PY{p}{)}
         
             \PY{n}{cur\PYZus{}feature} \PY{o}{=} \PY{n}{feature\PYZus{}scale}\PY{p}{[}\PY{n}{cur\PYZus{}header}\PY{p}{]}
             \PY{k}{for} \PY{n}{train\PYZus{}idx}\PY{p}{,}\PY{n}{test\PYZus{}idx} \PY{o+ow}{in} \PY{n}{kf}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{n}{cur\PYZus{}feature}\PY{p}{)}\PY{p}{:}
                 \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{cur\PYZus{}feature}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{n}{train\PYZus{}idx}\PY{p}{]} 
                 \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{label}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{n}{train\PYZus{}idx}\PY{p}{]}       
         
                 \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{cur\PYZus{}feature}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{n}{test\PYZus{}idx}\PY{p}{]}
                 \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{label}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{n}{test\PYZus{}idx}\PY{p}{]} 
         
                 \PY{n}{cur\PYZus{}model} \PY{o}{=} \PY{n}{GaussianNB}\PY{p}{(}\PY{n}{var\PYZus{}smoothing}\PY{o}{=}\PY{l+m+mf}{0.001}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
                 \PY{n}{y\PYZus{}predict} \PY{o}{=} \PY{n}{cur\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
                 \PY{n}{cur\PYZus{}accuracy} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}predict}\PY{p}{)}
         
                 \PY{k}{if} \PY{p}{(}\PY{n}{cur\PYZus{}accuracy} \PY{o}{\PYZgt{}} \PY{n}{best\PYZus{}accuracy\PYZus{}nb}\PY{p}{)}\PY{p}{:}
                     \PY{n}{best\PYZus{}model\PYZus{}nb} \PY{o}{=} \PY{n}{cur\PYZus{}model}
                     \PY{n}{best\PYZus{}accuracy\PYZus{}nb} \PY{o}{=} \PY{n}{cur\PYZus{}accuracy}
                     \PY{n}{best\PYZus{}header\PYZus{}nb} \PY{o}{=} \PY{n}{cur\PYZus{}header}
                     \PY{n}{best\PYZus{}label\PYZus{}nb} \PY{o}{=} \PY{n}{y\PYZus{}test}
                     \PY{n}{best\PYZus{}predict\PYZus{}nb} \PY{o}{=} \PY{n}{y\PYZus{}predict}
             
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Choosen feature: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{best\PYZus{}header\PYZus{}nb}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{best\PYZus{}accuracy\PYZus{}nb}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Precission: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{precision\PYZus{}score}\PY{p}{(}\PY{n}{best\PYZus{}label\PYZus{}nb}\PY{p}{,}\PY{n}{best\PYZus{}predict\PYZus{}nb}\PY{p}{,}\PY{n}{average}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{micro}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Recall: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{recall\PYZus{}score}\PY{p}{(}\PY{n}{best\PYZus{}label\PYZus{}nb}\PY{p}{,}\PY{n}{best\PYZus{}predict\PYZus{}nb}\PY{p}{,}\PY{n}{average}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{micro}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Choosen feature:  ['Column1', 'Column3', 'Column4', 'Column5', 'Column7', 'Column9', 'Column10', 'Column11', 'Column12', 'Column13']
Accuracy:  0.75
Precission:  0.75
Recall:  0.75

    \end{Verbatim}

    \subsubsection{Decision Tree ID3}\label{decision-tree-id3}

    Decision Tree yang digunakan pada eksplorasi ini adalah Decision Tree
ID3 dengan mengeset parameter criterion menjadi entropy. Pembangunan
model menggunakan K-Fold dengan 10 Fold sambil mencari kombinasi feature
yang mengoptimalkan accuracy model. Untuk Decision Tree, jumlah feature
yang mengoptimalkan kinerja adalah 7 yaitu {[}'Column2', 'Column3',
'Column4', 'Column5', 'Column6', 'Column8', 'Column9'{]}. Akurasi dari
model Decisin Tree ID3 yang didapatkan yaitu 73\%

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}80}]:} \PY{n}{kf} \PY{o}{=} \PY{n}{KFold}\PY{p}{(}\PY{n}{n\PYZus{}splits}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
         
         \PY{n}{best\PYZus{}model\PYZus{}dt} \PY{o}{=} \PY{k+kc}{None}
         \PY{n}{best\PYZus{}accuracy\PYZus{}dt} \PY{o}{=} \PY{l+m+mf}{0.0}
         \PY{n}{best\PYZus{}header\PYZus{}dt} \PY{o}{=} \PY{k+kc}{None}
         
         \PY{n}{best\PYZus{}label\PYZus{}dt} \PY{o}{=} \PY{k+kc}{None}
         \PY{n}{best\PYZus{}predict\PYZus{}dt} \PY{o}{=} \PY{k+kc}{None}
         
         \PY{n}{i} \PY{o}{=} \PY{l+m+mi}{6}    \PY{c+c1}{\PYZsh{} jumlah feature yang digunakan}
         \PY{n}{s} \PY{o}{=} \PY{l+m+mi}{930}
         \PY{n}{e} \PY{o}{=} \PY{l+m+mi}{931}
         
         \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{s}\PY{p}{,}\PY{n}{e}\PY{p}{)}\PY{p}{:}
             \PY{n}{cur\PYZus{}header} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{feature\PYZus{}combinations}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                 \PY{n}{cur\PYZus{}header}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{feature\PYZus{}combinations}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{[}\PY{n}{k}\PY{p}{]}\PY{p}{)}
         
             \PY{n}{cur\PYZus{}feature} \PY{o}{=} \PY{n}{feature\PYZus{}scale}\PY{p}{[}\PY{n}{cur\PYZus{}header}\PY{p}{]}
             \PY{k}{for} \PY{n}{train\PYZus{}idx}\PY{p}{,}\PY{n}{test\PYZus{}idx} \PY{o+ow}{in} \PY{n}{kf}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{n}{cur\PYZus{}feature}\PY{p}{)}\PY{p}{:}
                 \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{cur\PYZus{}feature}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{n}{train\PYZus{}idx}\PY{p}{]} 
                 \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{label}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{n}{train\PYZus{}idx}\PY{p}{]}       
         
                 \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{cur\PYZus{}feature}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{n}{test\PYZus{}idx}\PY{p}{]}
                 \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{label}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{n}{test\PYZus{}idx}\PY{p}{]} 
         
                 \PY{n}{cur\PYZus{}model} \PY{o}{=} \PY{n}{tree}\PY{o}{.}\PY{n}{DecisionTreeClassifier}\PY{p}{(}
                     \PY{n}{criterion}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{entropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                     \PY{n}{min\PYZus{}samples\PYZus{}leaf}\PY{o}{=}\PY{l+m+mi}{34}\PY{p}{,} 
                     \PY{n}{max\PYZus{}depth}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} 
                 \PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
                 
                 \PY{n}{y\PYZus{}predict} \PY{o}{=} \PY{n}{cur\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
                 \PY{n}{cur\PYZus{}accuracy} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}predict}\PY{p}{)}
         
                 \PY{k}{if} \PY{p}{(}\PY{n}{cur\PYZus{}accuracy} \PY{o}{\PYZgt{}} \PY{n}{best\PYZus{}accuracy\PYZus{}dt}\PY{p}{)}\PY{p}{:}
                     \PY{n}{best\PYZus{}model\PYZus{}dt} \PY{o}{=} \PY{n}{cur\PYZus{}model}
                     \PY{n}{best\PYZus{}accuracy\PYZus{}dt} \PY{o}{=} \PY{n}{cur\PYZus{}accuracy}
                     \PY{n}{best\PYZus{}header\PYZus{}dt} \PY{o}{=} \PY{n}{cur\PYZus{}header}
                     \PY{n}{best\PYZus{}label\PYZus{}dt} \PY{o}{=} \PY{n}{y\PYZus{}test}
                     \PY{n}{best\PYZus{}predict\PYZus{}dt} \PY{o}{=} \PY{n}{y\PYZus{}predict}
             
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Choosen feature: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{best\PYZus{}header\PYZus{}dt}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{best\PYZus{}accuracy\PYZus{}dt}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Precission: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{precision\PYZus{}score}\PY{p}{(}\PY{n}{best\PYZus{}label\PYZus{}dt}\PY{p}{,}\PY{n}{best\PYZus{}predict\PYZus{}dt}\PY{p}{,}\PY{n}{average}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{micro}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Recall: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{recall\PYZus{}score}\PY{p}{(}\PY{n}{best\PYZus{}label\PYZus{}dt}\PY{p}{,}\PY{n}{best\PYZus{}predict\PYZus{}dt}\PY{p}{,}\PY{n}{average}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{micro}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Choosen feature:  ['Column2', 'Column3', 'Column4', 'Column5', 'Column6', 'Column8', 'Column9']
Accuracy:  0.7333333333333333
Precission:  0.7333333333333333
Recall:  0.7333333333333333

    \end{Verbatim}

    \subsubsection{Multi Layer Perceptron}\label{multi-layer-perceptron}

Isu pertama dalam optimasi model MLP adalah menentukan
parameter-parameter MLPClassifier. Untuk parameter solver, kami memilih
lbfgs yang cocok digunakan untuk data berukuran kecil. Untuk parameter
activation kami memilih logistic (sigmoid) karena itu yang sudah
diajarkan di kelas. Dan random\_state diisi 100 (bebas, asalkan bukan
false) agar parameter-parameter initialization seperti weight, bias, dll
sama. Sedangkan parameter alpha, hidden\_layer\_sizes, dan
learning\_rates kami coba mencari kombinasi yang terbaik dari beberapa
nilai yang kami tawarkan. Hasilnya hidden\_layer\_sizes = (7, 11, 7),
learning\_rates = constant, dan alpha = 0.001.

Isu optimasi selanjutnya adalah pemilihan feature yang digunakan. Dengan
pertimbangan jumlah feature yang tidak terlalu banyak, dalam pemilihan
feature kami mencari kombinasi feature terbaik dari keseluruhan
kombinasi yang ada. Kami juga menggunakan strategi K-Fold untuk mencari
dataset yang menghasilkan akurasi paling baik.

Untuk MLP, jumlah feature yang menghasilkan akurasi model MLP terbaik
adalah 5 yaitu {[}'Column4', 'Column5', 'Column6', 'Column11',
'Column12'{]} . Akurasi dari model ini adalah 81\%

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}78}]:} \PY{n}{kf} \PY{o}{=} \PY{n}{KFold}\PY{p}{(}\PY{n}{n\PYZus{}splits}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
         
         \PY{n}{best\PYZus{}model\PYZus{}mlp} \PY{o}{=} \PY{k+kc}{None}
         \PY{n}{best\PYZus{}accuracy\PYZus{}mlp} \PY{o}{=} \PY{l+m+mf}{0.0}
         \PY{n}{best\PYZus{}header\PYZus{}mlp} \PY{o}{=} \PY{k+kc}{None}
         
         \PY{n}{best\PYZus{}label\PYZus{}mlp} \PY{o}{=} \PY{k+kc}{None}
         \PY{n}{best\PYZus{}predict\PYZus{}mlp} \PY{o}{=} \PY{k+kc}{None}
         
         \PY{n}{i} \PY{o}{=} \PY{l+m+mi}{4}
         \PY{n}{s} \PY{o}{=} \PY{l+m+mi}{1053}
         \PY{n}{e} \PY{o}{=} \PY{l+m+mi}{1054}
         \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{s}\PY{p}{,}\PY{n}{e}\PY{p}{)}\PY{p}{:}
             \PY{n}{cur\PYZus{}header} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{feature\PYZus{}combinations}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                 \PY{n}{cur\PYZus{}header}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{feature\PYZus{}combinations}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{[}\PY{n}{k}\PY{p}{]}\PY{p}{)}
                 
             \PY{n}{cur\PYZus{}feature} \PY{o}{=} \PY{n}{feature\PYZus{}scale}\PY{p}{[}\PY{n}{cur\PYZus{}header}\PY{p}{]}
             \PY{k}{for} \PY{n}{train\PYZus{}idx}\PY{p}{,}\PY{n}{test\PYZus{}idx} \PY{o+ow}{in} \PY{n}{kf}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{n}{cur\PYZus{}feature}\PY{p}{)}\PY{p}{:}
                 \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{cur\PYZus{}feature}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{n}{train\PYZus{}idx}\PY{p}{]} 
                 \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{label}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{n}{train\PYZus{}idx}\PY{p}{]}      
                 \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{cur\PYZus{}feature}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{n}{test\PYZus{}idx}\PY{p}{]}
                 \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{label}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{n}{test\PYZus{}idx}\PY{p}{]} 
         
                 \PY{n}{list\PYZus{}hidden\PYZus{}layer\PYZus{}sizes} \PY{o}{=} \PY{p}{[}\PY{p}{(}\PY{l+m+mi}{7}\PY{p}{,}\PY{l+m+mi}{11}\PY{p}{,}\PY{l+m+mi}{7}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{]}
                 \PY{n}{list\PYZus{}alpha} \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{0.0001}\PY{p}{,} \PY{l+m+mf}{0.001}\PY{p}{]}
                 \PY{n}{list\PYZus{}learning\PYZus{}rate} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{constant}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{adaptive}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         
                 \PY{k}{for} \PY{n}{hidden\PYZus{}layer\PYZus{}sizes\PYZus{}} \PY{o+ow}{in} \PY{n}{list\PYZus{}hidden\PYZus{}layer\PYZus{}sizes}\PY{p}{:}
                     \PY{k}{for} \PY{n}{alpha\PYZus{}} \PY{o+ow}{in} \PY{n}{list\PYZus{}alpha}\PY{p}{:}
                         \PY{k}{for} \PY{n}{learning\PYZus{}rate\PYZus{}} \PY{o+ow}{in} \PY{n}{list\PYZus{}learning\PYZus{}rate}\PY{p}{:}                
                             \PY{n}{cur\PYZus{}model} \PY{o}{=} \PY{n}{MLPClassifier}\PY{p}{(}\PY{n}{solver}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lbfgs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{logistic}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{n}{alpha\PYZus{}}\PY{p}{,}
                                              \PY{n}{hidden\PYZus{}layer\PYZus{}sizes}\PY{o}{=}\PY{n}{hidden\PYZus{}layer\PYZus{}sizes\PYZus{}}\PY{p}{,}  \PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{n}{learning\PYZus{}rate\PYZus{}}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
                             \PY{n}{y\PYZus{}predict} \PY{o}{=} \PY{n}{cur\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
                             \PY{n}{cur\PYZus{}accuracy} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}predict}\PY{p}{)}
         
                             \PY{k}{if} \PY{p}{(}\PY{n}{cur\PYZus{}accuracy} \PY{o}{\PYZgt{}} \PY{n}{best\PYZus{}accuracy\PYZus{}mlp}\PY{p}{)}\PY{p}{:}
                                 \PY{n}{best\PYZus{}model\PYZus{}mlp} \PY{o}{=} \PY{n}{cur\PYZus{}model}
                                 \PY{n}{best\PYZus{}accuracy\PYZus{}mlp} \PY{o}{=} \PY{n}{cur\PYZus{}accuracy}
                                 \PY{n}{best\PYZus{}header\PYZus{}mlp} \PY{o}{=} \PY{n}{cur\PYZus{}header}
                                 \PY{n}{best\PYZus{}parameter} \PY{o}{=} \PY{p}{\PYZob{}}
                                     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{alpha}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{n}{alpha\PYZus{}}\PY{p}{,}
                                     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hidden\PYZus{}layer\PYZus{}sizes}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{n}{hidden\PYZus{}layer\PYZus{}sizes\PYZus{}}\PY{p}{,}
                                     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{learning\PYZus{}rate}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{n}{learning\PYZus{}rate\PYZus{}}
                                 \PY{p}{\PYZcb{}}
                                 \PY{n}{best\PYZus{}label\PYZus{}mlp} \PY{o}{=} \PY{n}{y\PYZus{}test}
                                 \PY{n}{best\PYZus{}predict\PYZus{}mlp} \PY{o}{=} \PY{n}{y\PYZus{}predict}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Choosen feature: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{best\PYZus{}header\PYZus{}mlp}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Choosen parameter: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{best\PYZus{}parameter}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{best\PYZus{}accuracy\PYZus{}mlp}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Precission: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{precision\PYZus{}score}\PY{p}{(}\PY{n}{best\PYZus{}label\PYZus{}mlp}\PY{p}{,}\PY{n}{best\PYZus{}predict\PYZus{}mlp}\PY{p}{,}\PY{n}{average}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{micro}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Recall: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{recall\PYZus{}score}\PY{p}{(}\PY{n}{best\PYZus{}label\PYZus{}mlp}\PY{p}{,}\PY{n}{best\PYZus{}predict\PYZus{}mlp}\PY{p}{,}\PY{n}{average}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{micro}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Choosen feature:  ['Column4', 'Column5', 'Column6', 'Column11', 'Column12']
Choosen parameter:  \{'alpha': 0.001, 'hidden\_layer\_sizes': (7, 11, 7), 'learning\_rate': 'constant'\}
Accuracy:  0.8166666666666667
Precission:  0.8166666666666667
Recall:  0.8166666666666667

    \end{Verbatim}

    \subsubsection{Analisis Model}\label{analisis-model}

    Berdasarkan hasil eksperimen menggunakan model knn, naive bayes, dt, dan
mlp, model mlp menghasilkan statistik terbaik dengan akurasi, presisi,
recall masing-masing 81.67\%. Dilihat dari segi feature yang dipakai
masing-masing model, model mlp memiliki jumlah feature yang paling
sedikit diantara model yang lain yaitu 5. Sehingga, model mlp adalah
model yang paling general diantara yang lain. Oleh karena itu, model
terbaik dalam tugas kali ini adalah model mlp.

    \subsubsection{Save Best Model}\label{save-best-model}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}105}]:} \PY{n}{joblib}\PY{o}{.}\PY{n}{dump}\PY{p}{(}\PY{n}{best\PYZus{}model\PYZus{}mlp}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{best\PYZus{}model\PYZus{}mlp.pkl}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} 
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}105}]:} ['best\_model\_mlp.pkl']
\end{Verbatim}
            
    \subsubsection{Predict Data Test}\label{predict-data-test}

    Data test diambil dari file tubes2\_HeartDisease\_test.csv disimpan pada
dataframe pandas.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}86}]:} \PY{n}{file\PYZus{}test} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{tubes2\PYZus{}HeartDisease\PYZus{}test.csv}\PY{l+s+s2}{\PYZdq{}}
         \PY{n}{df\PYZus{}test} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{n}{file\PYZus{}test}\PY{p}{)}
         
         \PY{n}{feature\PYZus{}test} \PY{o}{=} \PY{n}{df\PYZus{}test}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}88}]:} \PY{n}{feature\PYZus{}impute\PYZus{}test} \PY{o}{=} \PY{n}{feature\PYZus{}test}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{?}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{nan}\PY{p}{)}
         
         \PY{n}{imputer\PYZus{}mode}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{feature\PYZus{}impute\PYZus{}test}\PY{p}{[}\PY{n}{discrete\PYZus{}value}\PY{p}{]}\PY{p}{)}
         \PY{n}{feature\PYZus{}impute\PYZus{}test}\PY{p}{[}\PY{n}{discrete\PYZus{}value}\PY{p}{]} \PY{o}{=} \PY{n}{imputer\PYZus{}mode}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{feature\PYZus{}impute\PYZus{}test}\PY{p}{[}\PY{n}{discrete\PYZus{}value}\PY{p}{]}\PY{p}{)}
         
         \PY{n}{imputer\PYZus{}mean}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{feature\PYZus{}impute\PYZus{}test}\PY{p}{[}\PY{n}{continues\PYZus{}value}\PY{p}{]}\PY{p}{)}
         \PY{n}{feature\PYZus{}impute\PYZus{}test}\PY{p}{[}\PY{n}{continues\PYZus{}value}\PY{p}{]} \PY{o}{=} \PY{n}{imputer\PYZus{}mean}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{feature\PYZus{}impute\PYZus{}test}\PY{p}{[}\PY{n}{continues\PYZus{}value}\PY{p}{]}\PY{p}{)}
         
         \PY{n}{feature\PYZus{}impute\PYZus{}test}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Column13}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{to\PYZus{}numeric}\PY{p}{(}\PY{n}{feature\PYZus{}impute\PYZus{}test}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Column13}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{feature\PYZus{}impute\PYZus{}test}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}88}]:}    Column1  Column2  Column3  Column4  Column5 Column6  Column7  Column8  \textbackslash{}
         0       60        1        2    160.0    267.0       1        1    157.0   
         1       61        1        4    148.0    203.0       0        0    161.0   
         2       54        1        4    130.0    242.0       0        0     91.0   
         3       48        1        4    120.0    260.0       0        0    115.0   
         4       57        0        1    130.0    308.0       0        0     98.0   
         
           Column9  Column10  Column11  Column12  Column13  
         0       0       0.5       2.0  0.613636         7  
         1       0       0.0       1.0  1.000000         7  
         2       1       1.0       2.0  0.613636         7  
         3       0       2.0       2.0  0.613636         7  
         4       0       1.0       2.0  0.613636         7  
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}90}]:} \PY{n}{feature\PYZus{}scale\PYZus{}test} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{preprocessing}\PY{o}{.}\PY{n}{scale}\PY{p}{(}\PY{n}{feature\PYZus{}impute\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{n}{header}\PY{p}{)}
         \PY{n}{feature\PYZus{}scale\PYZus{}test}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/home/ds/anaconda3/lib/python3.6/site-packages/ipykernel\_launcher.py:1: DataConversionWarning: Data with input dtype int64, float64, object were all converted to float64 by the scale function.
  """Entry point for launching an IPython kernel.

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}90}]:}     Column1   Column2   Column3   Column4   Column5   Column6   Column7  \textbackslash{}
         0  0.722781  0.429970 -1.228609  1.634929  0.660909  2.054805  0.495034   
         1  0.834283  0.429970  0.871196  0.961467  0.092894 -0.486664 -0.774053   
         2  0.053774  0.429970  0.871196 -0.048726  0.439028 -0.486664 -0.774053   
         3 -0.615234  0.429970  0.871196 -0.609944  0.598783 -0.486664 -0.774053   
         4  0.388278 -2.325745 -2.278512 -0.048726  1.024794 -0.486664 -0.774053   
         
             Column8   Column9  Column10  Column11  Column12  Column13  
         0  1.019460 -0.729800 -0.424101  0.368695  0.000000   0.47701  
         1  1.189424 -0.729800 -0.488818 -1.669972  0.740049   0.47701  
         2 -1.784953  1.370238 -0.359384  0.368695  0.000000   0.47701  
         3 -0.765167 -0.729800 -0.229949  0.368695  0.000000   0.47701  
         4 -1.487515 -0.729800 -0.359384  0.368695  0.000000   0.47701  
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}106}]:} \PY{n}{model} \PY{o}{=} \PY{n}{joblib}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{best\PYZus{}model\PYZus{}mlp.pkl}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
          
          \PY{n}{y\PYZus{}predict\PYZus{}test} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{feature\PYZus{}scale\PYZus{}test}\PY{p}{[}\PY{n}{best\PYZus{}header\PYZus{}mlp}\PY{p}{]}\PY{p}{)}
          
          \PY{n}{y\PYZus{}predict\PYZus{}test}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}106}]:} array([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
                 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
                 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0,
                 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
                 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
                 0, 0, 1, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
                 0, 0, 1, 1, 0, 0, 0, 0, 0])
\end{Verbatim}
            

    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
